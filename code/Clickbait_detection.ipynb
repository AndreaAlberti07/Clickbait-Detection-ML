{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clickbait Detection\n",
    "## Machine Learning Course Project\n",
    "---\n",
    "### Author: Andrea Alberti  \n",
    "### Date: June 2023\n",
    "---\n",
    "## Data: \n",
    "The collected dataset, includes 32 000 headlines, equally\n",
    "divided in the ‘clickbait’ and ‘non-clickbait’ classes. \n",
    "\n",
    "It is split into training, validation, and test sets consisting of 24 000, 4000 and 4000 samples, respectively. \n",
    "\n",
    "The data are stored in text files, with one headline for each line.\n",
    "\n",
    "\n",
    "## Goal:\n",
    "Build classifiers for the detection of the clickbait headlines. Consider two scenarios: accuracy oriented and fpr oriented.\n",
    "\n",
    "---\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import string\n",
    "import CD_functions as cdf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn.metrics as skm\n",
    "import pvml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1 - **DATA ANALYSIS**\n",
    "\n",
    "The data are analyzed in order to understand how they are structured and how they can be handled.\n",
    "\n",
    "Notations:\n",
    "\n",
    "- The test_bait and train_bait files contain some headlines starting with #, so they are loaded specifying the comment character if using np.loadtxt.\n",
    "- All the others files do not present the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset to check the dimensionality of the data\n",
    "\n",
    "#CLICKBAIT (CLASS 1)\n",
    "test_bait = np.loadtxt('data/clickbait_test.txt', dtype=str, delimiter='\\n', comments=None)\n",
    "print('test_bait: ', test_bait.shape)\n",
    "train_bait = np.loadtxt('data/clickbait_train.txt', dtype=str, delimiter='\\n', comments=None)\n",
    "print('train_bait: ', train_bait.shape)\n",
    "validation_bait = np.loadtxt('data/clickbait_validation.txt', dtype=str, delimiter='\\n')\n",
    "print('validation_bait: ', validation_bait.shape)\n",
    "\n",
    "#NON-CLICKBAIT (CLASS 0)\n",
    "test_nobait = np.loadtxt('data/non_clickbait_test.txt', dtype=str, delimiter='\\n')\n",
    "print('test_nobait: ', test_nobait.shape)\n",
    "train_nobait = np.loadtxt('data/non_clickbait_train.txt', dtype=str, delimiter='\\n')\n",
    "print('train_nobait: ', train_nobait.shape)\n",
    "validation_nobait = np.loadtxt('data/non_clickbait_validation.txt', dtype=str, delimiter='\\n')\n",
    "print('validation_nobait: ', validation_nobait.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2 - **DATA PRE-PROCESSING**\n",
    "\n",
    "The data are pre-processed, extracting the features from the headlines, creating the BoW representation.\n",
    "\n",
    "Create Vocabulary:\n",
    "- **Tokenization**: the headlines are split into tokens, removing the punctuation and the stopwords.\n",
    "\n",
    "Create the BoW representation:\n",
    "- **CountVectorizer**: the headlines are represented as a matrix, where each row is a headline and each column is a token. The value of each cell is the number of times the token appears in the headline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXECUTE JUST ONE TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCABULARY WITHOUT STOPWORDS\n",
    "\n",
    "# Define the vocsizes\n",
    "vocsizes = [100, 200, 400, 800, 1000, 2000, 4000, 8000]\n",
    "\n",
    "# Create the vocabulary\n",
    "text_list = [train_bait, train_nobait]\n",
    "for size in vocsizes:\n",
    "    size = str(size)\n",
    "    vocabulary = cdf.create_vocabulary(text_list, int(size), 'generated_gitignore/vocabulary_stop'+size+'.txt', store=True, remove_stopwords=True)\n",
    "\n",
    "# Generate the BoWs\n",
    "for size in vocsizes:\n",
    "    size = str(size)\n",
    "    \n",
    "    # Load the vocabulary\n",
    "    vocabulary = open('generated_gitignore/vocabulary_stop'+size+'.txt', 'r').read().lower().split()\n",
    "\n",
    "    # Create BoW\n",
    "    #CLICKBAIT\n",
    "    bow_train_bait = cdf.create_bow(train_bait, vocabulary, 1)\n",
    "    print('bow_train_bait_stop'+size+': ', bow_train_bait.shape)\n",
    "    np.savetxt('generated_gitignore/bow_train_bait_stop'+size+'.txt.gz', bow_train_bait)\n",
    "    bow_test_bait = cdf.create_bow(test_bait, vocabulary, 1)\n",
    "    print('bow_test_bait_stop'+size+': ', bow_test_bait.shape)\n",
    "    np.savetxt('generated_gitignore/bow_test_bait_stop'+size+'.txt.gz', bow_test_bait)\n",
    "    bow_validation_bait = cdf.create_bow(validation_bait, vocabulary, 1)\n",
    "    print('bow_validation_bait_stop'+size+': ', bow_validation_bait.shape)\n",
    "    np.savetxt('generated_gitignore/bow_validation_bait_stop'+size+'.txt.gz', bow_validation_bait)\n",
    "\n",
    "    #NON-CLICKBAIT\n",
    "    bow_train_nobait = cdf.create_bow(train_nobait, vocabulary, 0)\n",
    "    print('bow_train_nobait_stop'+size+': ', bow_train_nobait.shape)\n",
    "    np.savetxt('generated_gitignore/bow_train_nobait_stop'+size+'.txt.gz', bow_train_nobait)\n",
    "    bow_test_nobait = cdf.create_bow(test_nobait, vocabulary, 0)\n",
    "    print('bow_test_nobait_stop'+size+': ', bow_test_nobait.shape)\n",
    "    np.savetxt('generated_gitignore/bow_test_nobait_stop'+size+'.txt.gz', bow_test_nobait)\n",
    "    bow_validation_nobait = cdf.create_bow(validation_nobait, vocabulary, 0)\n",
    "    print('bow_validation_nobait_stop'+size+': ', bow_validation_nobait.shape)\n",
    "    np.savetxt('generated_gitignore/bow_validation_nobait_stop'+size+'.txt.gz', bow_validation_nobait)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCABULARY WITH STOPWORDS\n",
    "\n",
    "# Define the vocsizes\n",
    "vocsizes = [100, 200, 400, 800, 1000, 2000, 4000, 8000]\n",
    "\n",
    "# Create the vocabulary\n",
    "text_list = [train_bait, train_nobait]\n",
    "for size in vocsizes:\n",
    "    size = str(size)\n",
    "    vocabulary = cdf.create_vocabulary(text_list, int(size), 'generated_gitignore/vocabulary_NOstop'+size+'.txt', store=True, remove_stopwords=False)\n",
    "\n",
    "# Generate the BoWs\n",
    "for size in vocsizes:\n",
    "    size = str(size)\n",
    "    \n",
    "    # Load the vocabulary\n",
    "    vocabulary = open('generated_gitignore/vocabulary_NOstop'+size+'.txt', 'r').read().lower().split()\n",
    "\n",
    "    # Create BoW\n",
    "    #CLICKBAIT\n",
    "    bow_train_bait = cdf.create_bow(train_bait, vocabulary, 1)\n",
    "    print('bow_train_bait_NOstop'+size+': ', bow_train_bait.shape)\n",
    "    np.savetxt('generated_gitignore/bow_train_bait_NOstop'+size+'.txt.gz', bow_train_bait)\n",
    "    bow_test_bait = cdf.create_bow(test_bait, vocabulary, 1)\n",
    "    print('bow_test_bait_NOstop'+size+': ', bow_test_bait.shape)\n",
    "    np.savetxt('generated_gitignore/bow_test_bait_NOstop'+size+'.txt.gz', bow_test_bait)\n",
    "    bow_validation_bait = cdf.create_bow(validation_bait, vocabulary, 1)\n",
    "    print('bow_validation_bait_NOstop'+size+': ', bow_validation_bait.shape)\n",
    "    np.savetxt('generated_gitignore/bow_validation_bait_NOstop'+size+'.txt.gz', bow_validation_bait)\n",
    "\n",
    "    #NON-CLICKBAIT\n",
    "    bow_train_nobait = cdf.create_bow(train_nobait, vocabulary, 0)\n",
    "    print('bow_train_nobait_NOstop'+size+': ', bow_train_nobait.shape)\n",
    "    np.savetxt('generated_gitignore/bow_train_nobait_NOstop'+size+'.txt.gz', bow_train_nobait)\n",
    "    bow_test_nobait = cdf.create_bow(test_nobait, vocabulary, 0)\n",
    "    print('bow_test_nobait_NOstop'+size+': ', bow_test_nobait.shape)\n",
    "    np.savetxt('generated_gitignore/bow_test_nobait_NOstop'+size+'.txt.gz', bow_test_nobait)\n",
    "    bow_validation_nobait = cdf.create_bow(validation_nobait, vocabulary, 0)\n",
    "    print('bow_validation_nobait_NOstop'+size+': ', bow_validation_nobait.shape)\n",
    "    np.savetxt('generated_gitignore/bow_validation_nobait_NOstop'+size+'.txt.gz', bow_validation_nobait)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3 - **CLASSIFICATION MODELS**\n",
    "\n",
    "- 3.1 - **Multinomial Naive Bayes**\n",
    "- 3.2 - **Logistic Regression**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 - **Multinomial Naive Bayes**\n",
    "\n",
    "- **Accuracy**: for different vocabulary sizes, the accuracy is computed on the validation set.\n",
    "- **Confusion Matrix**: for each vocabulary size, are computed fpr and accuracy on the validation set.\n",
    "- **ROC Curve**: for each vocabulary size, are computed fpr and tpr on the validation set.\n",
    "- **Lowest FPR**: for each vocabulary size, the lowest fpr is computed on the validation set and plotted with the accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 - **Stopwords Removed** (file stop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- COMPUTE THE ACCURACY ON THE VALIDATION SET FOR DIFFERENT VOCABULARY SIZES\n",
    ">- COMPUTE THE ROC CURVE ON THE VALIDATION SET FOR DIFFERENT VOCABULARY SIZES\n",
    ">- STORE FOR EACH VOCABULARY SIZE THE LOWEST FPR AND THE ACCURACY ON THE VALIDATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocsizes\n",
    "vocsizes = [100, 200, 400, 800, 1000, 2000, 4000, 8000]\n",
    "fprs_sizes = []\n",
    "tprs_sizes = []\n",
    "\n",
    "# Define the range for the biases\n",
    "b_pos = np.linspace(-5, 5, 50)\n",
    "b_neg = np.linspace(5, -5, 50)\n",
    "biases = np.vstack((b_neg, b_pos)).T\n",
    "\n",
    "# Create file where to store accs\n",
    "with open('results/accuracies_mnbc_stop.csv', 'w') as f: #overwrite the file\n",
    "    f.write('vocsize,accuracy_train,accuracy_validation\\n')\n",
    "    \n",
    "# Create file where to store data for the lowest fpr\n",
    "with open('results/low_fpr_mnbc_stop.csv', 'w') as f:\n",
    "    f.write('vocsize,bias_neg,bias_pos,fpr,tpr,acc\\n')\n",
    "    \n",
    "for size in vocsizes:\n",
    "    size = str(size)\n",
    "\n",
    "    # Load the BoW for training (vstack on the two classes)\n",
    "    bow_train_bait = np.loadtxt('generated_gitignore/bow_train_bait_stop'+size+'.txt.gz')\n",
    "    bow_train_nobait = np.loadtxt('generated_gitignore/bow_train_nobait_stop'+size+'.txt.gz')\n",
    "    bow_train = np.vstack((bow_train_bait, bow_train_nobait))\n",
    "\n",
    "    bow_validation_bait = np.loadtxt('generated_gitignore/bow_validation_bait_stop'+size+'.txt.gz')\n",
    "    bow_validation_nobait = np.loadtxt('generated_gitignore/bow_validation_nobait_stop'+size+'.txt.gz')\n",
    "    bow_validation = np.vstack((bow_validation_bait, bow_validation_nobait))\n",
    "\n",
    "    # Train the multinomial Naive Bayes classifier\n",
    "    w, b = pvml.multinomial_naive_bayes_train(bow_train[:, :-1], bow_train[:, -1])\n",
    "\n",
    "    # Use the classifier to predict the labels of the validation set changing the vocabulary size\n",
    "    predictions_val, scores_val = pvml.multinomial_naive_bayes_inference(bow_validation[:, :-1], w, b)\n",
    "    predictions_tra, scores_tra = pvml.multinomial_naive_bayes_inference(bow_train[:, :-1], w, b)\n",
    "\n",
    "    # Accuracy\n",
    "    with open('results/accuracies_mnbc_stop.csv', 'a') as f:\n",
    "        f.write(size+','+str(cdf.accuracy(predictions_tra, bow_train[:, -1]))+','+str(cdf.accuracy(predictions_val, bow_validation[:, -1]))+'\\n')\n",
    "    \n",
    "    # Compute the ROC curve\n",
    "    fprs, tprs, = cdf.roc_curve_biases(pvml.multinomial_naive_bayes_inference, bow_validation, w, biases, size, 'ROC curve MNBC for different biases (stopwords)', plot=True)\n",
    "    fprs_sizes.append(fprs)\n",
    "    tprs_sizes.append(tprs)\n",
    "    \n",
    "    # Find the biases associated with the lowest fpr\n",
    "    col = np.argmin(fprs)\n",
    "    pred, s = pvml.multinomial_naive_bayes_inference(bow_validation[:, :-1], w, biases[col, :])\n",
    "    \n",
    "    with open('results/low_fpr_mnbc_stop.csv', 'a') as f:\n",
    "        f.write(f'{size},{biases[col, 0]},{biases[col, 1]},{fprs[col]},{tprs[col]},{cdf.accuracy(pred, bow_validation[:, -1])}\\n')\n",
    "    \n",
    "\n",
    "# Save the FPRs (each row is a different vocabulary size, each column is a different bias. The first two rows are the biases, first negatives then positives)\n",
    "fprs_arr = np.stack(fprs_sizes, axis=0)\n",
    "fprs_arr = np.vstack((biases.T, fprs_arr))\n",
    "np.savetxt('results/fprs_mnbc_stop.txt.gz', fprs_arr)\n",
    "\n",
    "# Save the TPRs (each row is a different vocabulary size, each column is a different bias. The first two rows are the biases, first negatives then positives)\n",
    "tprs_arr = np.stack(tprs_sizes, axis=0)\n",
    "tprs_arr = np.vstack((biases.T, tprs_arr))\n",
    "np.savetxt('results/tprs_mnbc_stop.txt.gz', tprs_arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- SHOW THE ACCURACIES FOR DIFFERENT VOCABULARY SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the plot\n",
    "pd.read_csv('results/accuracies_mnbc_stop.csv').plot(x='vocsize', y=['accuracy_train', 'accuracy_validation'], kind='line', ylabel='Accuracy (%)', xlabel='Vocabulary size', title='Multinomial Naive Bayes Classifier (stopwords)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- SHOW THE RESULTS ABOUT THE LOWEST FPR STORED ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the results related to the lowest fpr\n",
    "cdf.fpr_accs_vocsizes('results/low_fpr_mnbc_stop.csv', 'MNBC FPR and Accuracy vs. Vocsize (stopwords)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 - **Stopwords Kept** (file NOstop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- COMPUTE THE ACCURACY ON THE VALIDATION SET FOR DIFFERENT VOCABULARY SIZES\n",
    ">- COMPUTE THE ROC CURVE ON THE VALIDATION SET FOR DIFFERENT VOCABULARY SIZES\n",
    ">- STORE FOR EACH VOCABULARY SIZE THE LOWEST FPR AND THE ACCURACY ON THE VALIDATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocsizes\n",
    "vocsizes = [100, 200, 400, 800, 1000, 2000, 4000, 8000]\n",
    "fprs_sizes = []\n",
    "tprs_sizes = []\n",
    "\n",
    "# Define the range for the biases\n",
    "b_pos = np.linspace(-5, 5, 50) #IF THE ROC CURVE IS NOT COMPLETE, JUST EXTEND THE RANGE\n",
    "b_neg = np.linspace(5, -5, 50) #IF THE ROC CURVE IS NOT COMPLETE, JUST EXTEND THE RANGE\n",
    "biases = np.vstack((b_neg, b_pos)).T\n",
    "\n",
    "# Create file where to store accs\n",
    "with open('results/accuracies_mnbc_NOstop.csv', 'w') as f: #overwrite the file\n",
    "    f.write('vocsize,accuracy_train,accuracy_validation\\n')\n",
    "    \n",
    "# Create file where to store data for the lowest fpr\n",
    "#with open('results/low_fpr_mnbc_NOstop.csv', 'w') as f:\n",
    "#    f.write('vocsize,bias_neg,bias_pos,fpr,tpr,acc\\n')\n",
    "    \n",
    "for size in vocsizes:\n",
    "    size = str(size)\n",
    "\n",
    "    # Load the BoW for training (vstack on the two classes)\n",
    "    bow_train_bait = np.loadtxt('generated_gitignore/bow_train_bait_NOstop'+size+'.txt.gz')\n",
    "    bow_train_nobait = np.loadtxt('generated_gitignore/bow_train_nobait_NOstop'+size+'.txt.gz')\n",
    "    bow_train = np.vstack((bow_train_bait, bow_train_nobait))\n",
    "\n",
    "    bow_validation_bait = np.loadtxt('generated_gitignore/bow_validation_bait_NOstop'+size+'.txt.gz')\n",
    "    bow_validation_nobait = np.loadtxt('generated_gitignore/bow_validation_nobait_NOstop'+size+'.txt.gz')\n",
    "    bow_validation = np.vstack((bow_validation_bait, bow_validation_nobait))\n",
    "\n",
    "    # Train the multinomial Naive Bayes classifier\n",
    "    w, b = pvml.multinomial_naive_bayes_train(bow_train[:, :-1], bow_train[:, -1])\n",
    "\n",
    "    # Use the classifier to predict the labels of the validation set changing the vocabulary size\n",
    "    predictions_val, scores_val = pvml.multinomial_naive_bayes_inference(bow_validation[:, :-1], w, b)\n",
    "    predictions_tra, scores_tra = pvml.multinomial_naive_bayes_inference(bow_train[:, :-1], w, b)\n",
    "\n",
    "    # Accuracy\n",
    "    with open('results/accuracies_mnbc_NOstop.csv', 'a') as f:\n",
    "        f.write(size+','+str(cdf.accuracy(predictions_tra, bow_train[:, -1]))+','+str(cdf.accuracy(predictions_val, bow_validation[:, -1]))+'\\n')\n",
    "    \n",
    "    # Compute the ROC curve\n",
    "    fprs, tprs, = cdf.roc_curve_biases(pvml.multinomial_naive_bayes_inference, bow_validation, w, biases, size, 'ROC curve MNBC for different biases', plot=True)\n",
    "    fprs_sizes.append(fprs)\n",
    "    tprs_sizes.append(tprs)\n",
    "    \n",
    "    # Find the biases associated with the lowest fpr\n",
    "    col = np.argmin(fprs)\n",
    "    pred, s = pvml.multinomial_naive_bayes_inference(bow_validation[:, :-1], w, biases[col, :])\n",
    "    \n",
    "    with open('results/low_fpr_mnbc_NOstop.csv', 'a') as f:\n",
    "        f.write(f'{size},{biases[col, 0]},{biases[col, 1]},{fprs[col]},{tprs[col]},{cdf.accuracy(pred, bow_validation[:, -1])}\\n')\n",
    "    \n",
    "\n",
    "# Save the FPRs (each row is a different vocabulary size, each column is a different bias. The first two rows are the biases, first negatives then positives)\n",
    "fprs_arr = np.stack(fprs_sizes, axis=0)\n",
    "fprs_arr = np.vstack((biases.T, fprs_arr))\n",
    "np.savetxt('results/fprs_mnbc_NOstop.txt.gz', fprs_arr)\n",
    "\n",
    "# Save the TPRs (each row is a different vocabulary size, each column is a different bias. The first two rows are the biases, first negatives then positives)\n",
    "tprs_arr = np.stack(tprs_sizes, axis=0)\n",
    "tprs_arr = np.vstack((biases.T, tprs_arr))\n",
    "np.savetxt('results/tprs_mnbc_NOstop.txt.gz', tprs_arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- SHOW THE ACCURACIES FOR DIFFERENT VOCABULARY SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the plot\n",
    "pd.read_csv('results/accuracies_mnbc_NOstop.csv').plot(x='vocsize', y=['accuracy_train', 'accuracy_validation'], kind='line', ylabel='Accuracy (%)', xlabel='Vocabulary size', title='Multinomial Naive Bayes Classifier')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- SHOW THE RESULTS ABOUT THE LOWEST FPR STORED ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the results related to the lowest fpr\n",
    "cdf.fpr_accs_vocsizes('results/low_fpr_mnbc_NOstop.csv', 'MNBC FPR and Accuracy vs. Vocsize')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 - **Logistic Regression**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- SELECTION OF THE BEST LEARNING RATE AMONG A LIST OF VALUES\n",
    ">- COMPUTE TRAIN ACCURACY, VALIDATION ACCURACY AND LOSS FOR EACH LEARNING RATE OF EACH DIFFERENT VOCABULARY SIZE (the vocabulary considered is just that without stopwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXECUTE JUST ONE TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocsizes\n",
    "vocsizes = [100, 200, 400, 800, 1000, 2000, 4000, 8000]\n",
    "learning_rates = [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1]\n",
    "fprs_sizes = []\n",
    "tprs_sizes = []\n",
    "\n",
    "train_accs_lrs = []\n",
    "test_accs_lrs = []\n",
    "losses_lrs = []\n",
    "ITC_lrs = []\n",
    "\n",
    "'''# Create file where to store accs\n",
    "with open('results/accuracies_lr.csv', 'w') as f: #overwrite the file\n",
    "    f.write('vocsize,accuracy_train,accuracy_validation,learning_rate\\n')'''\n",
    "\n",
    "for size in vocsizes:\n",
    "    size = str(size)\n",
    "\n",
    "    # Load the BoW for training (vstack on the two classes)\n",
    "    bow_train_bait = np.loadtxt('generated_gitignore/bow_train_bait_stop'+size+'.txt.gz')\n",
    "    bow_train_nobait = np.loadtxt('generated_gitignore/bow_train_nobait_stop'+size+'.txt.gz')\n",
    "    bow_train = np.vstack((bow_train_bait, bow_train_nobait))\n",
    "\n",
    "    bow_validation_bait = np.loadtxt('generated_gitignore/bow_validation_bait_stop'+size+'.txt.gz')\n",
    "    bow_validation_nobait = np.loadtxt('generated_gitignore/bow_validation_nobait_stop'+size+'.txt.gz')\n",
    "    bow_validation = np.vstack((bow_validation_bait, bow_validation_nobait))\n",
    "    \n",
    "    # Analyze different learning rates\n",
    "    for lr_ in learning_rates:\n",
    "        # Train the Logistic Regression\n",
    "        w, b, train_accuracies, test_accuracies, losses, ITC = cdf.logreg_training(bow_train[:, :-1], bow_train[:, -1], steps = 1000, lr = lr_, lambda_=0, X_test = bow_validation[:,:-1], Y_test = bow_validation[:,-1])\n",
    "        train_accs_lrs.append(train_accuracies)\n",
    "        test_accs_lrs.append(test_accuracies)\n",
    "        losses_lrs.append(losses)\n",
    "        ITC_lrs.append(ITC)\n",
    "        \n",
    "        # Use the classifier to predict the labels of the validation set changing the vocabulary size\n",
    "        prob_val = cdf.logreg_inference(bow_validation[:, :-1], w, b)\n",
    "        prob_tra = cdf.logreg_inference(bow_train[:, :-1], w, b)\n",
    "\n",
    "        '''# Accuracy\n",
    "        with open('results/accuracies_lr.csv', 'a') as f:\n",
    "            f.write(f'{size},{cdf.accuracy((prob_tra > 0.5).astype(int), bow_train[:, -1])},{cdf.accuracy((prob_val > 0.5).astype(int), bow_validation[:, -1])},{lr_}\\n')'''\n",
    "        print('size: ', size, 'lr: ', lr_)\n",
    "        \n",
    "    print('size completed: ', size)\n",
    "    \n",
    "# Each row is a different learning rate, each column is an amount of iterations (multiple of 100) until convergence (tol or number of steps)\n",
    "# The number of rows is 56, the first 7 are for the first vocsize, the second 7 for the second vocsize and so on\n",
    "\n",
    "np.savetxt('results/train_accs_lrs_stop.txt.gz', np.stack(train_accs_lrs, axis=0))\n",
    "np.savetxt('results/test_accs_lrs_stop.txt.gz', np.stack(test_accs_lrs, axis=0))\n",
    "np.savetxt('results/losses_lrs_stop.txt.gz', np.stack(losses_lrs, axis=0))\n",
    "np.savetxt('results/ITC_lrs_stop.txt.gz', ITC_lrs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- SHOW THE RESULTS ABOUT THE BEST LEARNING RATE FOR EACH VOCABULARY SIZE (the vocabulary considered is just that without stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1]\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for j, size in enumerate(vocsizes):\n",
    "    data = np.loadtxt('results/losses_lrs_stop.txt.gz') #change the name to change plot. Choose among (train_accs_lrs_stop, test_accs_lrs_stop, losses_lrs_stop, ITC_lrs_stop).\n",
    "    for i in range(j*7,(j+1)*7):\n",
    "        axs[j].plot(np.arange(len(data[i]))*100, data[i,:], label='lr: '+str(learning_rates[i-j*7]))\n",
    "        axs[j].title.set_text('vocsize: '+str(size))\n",
    "        \n",
    "fig.text(0.5, -0.02, 'Iterations', ha='center', va='center', fontsize=16)\n",
    "fig.text(-0.01, 0.5, 'Loss', ha='center', va='center', rotation='vertical', fontsize=16)\n",
    "fig.tight_layout()\n",
    "lines, labels = axs[0].get_legend_handles_labels()\n",
    "fig.legend(lines, labels, loc='upper right', ncols=7, bbox_to_anchor=(0.72, -0.035))\n",
    "fig.text(0.5, 1.01, 'Loss vs. Iterations for different learning rates', ha='center', va='center', fontsize=16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- COMPUTE THE **PARAMETERS** FOR ALL THE VOCSIZES IN THE LIST AND STORE THEM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXECUTE JUST ONE TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE THE PARAMETERS FOR THE DIFFERENT VOCSIZES\n",
    "\n",
    "vocsizes = [100, 200, 400, 800, 1000, 2000, 4000, 8000]\n",
    "best_lr = 0.001\n",
    "\n",
    "# Parameters for the vocabulary without stopwords\n",
    "cdf.train_logreg_vocsizes(vocsizes, best_lr, 2000, 'models_trained', remove_stopwords=True)\n",
    "\n",
    "# Parameters for the vocabulary with stopwords\n",
    "cdf.train_logreg_vocsizes(vocsizes, best_lr, 2000, 'models_trained', remove_stopwords=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 - **Stopwords Removed** (file stop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- COMPUTE THE ACCURACY ON THE VALIDATION SET FOR DIFFERENT VOCABULARY SIZES\n",
    ">- COMPUTE THE ROC CURVE ON THE VALIDATION SET FOR DIFFERENT VOCABULARY SIZES\n",
    ">- STORE FOR EACH VOCABULARY SIZE THE FPR CLOSEST TO A GIVEN THRESHOLD AND THE ACCURACY ON THE VALIDATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocsizes\n",
    "vocsizes = [100, 200, 400, 800, 1000, 2000, 4000, 8000]\n",
    "fprs_sizes = []\n",
    "tprs_sizes = []\n",
    "fpr_threshold = -0.1\n",
    "\n",
    "# Define the range for the biases\n",
    "biases = np.linspace(-8,8,1000) #IF THE ROC CURVE IS NOT COMPLETE, JUST EXTEND THE RANGE\n",
    "\n",
    "# Create file where to store accs\n",
    "with open('results/accuracies_lr_stop.csv', 'w') as f: #overwrite the file\n",
    "    f.write('vocsize,accuracy_train,accuracy_validation\\n')\n",
    "    \n",
    "# Create file where to store data for the lowest fpr\n",
    "with open('results/low_fpr_lr_stop.csv', 'w') as f:\n",
    "    f.write('vocsize,bias,fpr,tpr,acc\\n')\n",
    "    \n",
    "for size in vocsizes:\n",
    "    size = str(size)\n",
    "\n",
    "    # Load the BoW for training (vstack on the two classes)\n",
    "    bow_train_bait = np.loadtxt('generated_gitignore/bow_train_bait_stop'+size+'.txt.gz')\n",
    "    bow_train_nobait = np.loadtxt('generated_gitignore/bow_train_nobait_stop'+size+'.txt.gz')\n",
    "    bow_train = np.vstack((bow_train_bait, bow_train_nobait))\n",
    "\n",
    "    bow_validation_bait = np.loadtxt('generated_gitignore/bow_validation_bait_stop'+size+'.txt.gz')\n",
    "    bow_validation_nobait = np.loadtxt('generated_gitignore/bow_validation_nobait_stop'+size+'.txt.gz')\n",
    "    bow_validation = np.vstack((bow_validation_bait, bow_validation_nobait))\n",
    "\n",
    "    # Load the Logistic Regression parameters\n",
    "    params = np.load('models_trained/param_logreg_stop'+size+'.npz')\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "\n",
    "    # Use the classifier to predict the labels of the validation set changing the vocabulary size\n",
    "    probs_val = cdf.logreg_inference(bow_validation[:, :-1], w, b)\n",
    "    predictions_val = (probs_val > 0.5).astype(int)\n",
    "    probs_tra = cdf.logreg_inference(bow_train[:, :-1], w, b)\n",
    "    predictions_tra = (probs_tra > 0.5).astype(int)\n",
    "\n",
    "    # Accuracy\n",
    "    with open('results/accuracies_lr_stop.csv', 'a') as f:\n",
    "        f.write(size+','+str(cdf.accuracy(predictions_tra, bow_train[:, -1]))+','+str(cdf.accuracy(predictions_val, bow_validation[:, -1]))+'\\n')\n",
    "    \n",
    "    # Compute the ROC curve\n",
    "    fprs, tprs, = cdf.roc_curve_biases(cdf.logreg_inference, bow_validation, w, biases, size, 'ROC curve LR for different biases (stopwords)', plot=True)\n",
    "    fprs_sizes.append(fprs)\n",
    "    tprs_sizes.append(tprs)\n",
    "    \n",
    "    # Find the biases associated with the lowest fpr\n",
    "    fprs_tmp = np.array(fprs)\n",
    "    col = np.where(fprs_tmp>fpr_threshold)[0][0]\n",
    "    prob = cdf.logreg_inference(bow_validation[:, :-1], w, biases[col])\n",
    "    pred = (prob > 0.5).astype(int)\n",
    "    \n",
    "    with open('results/low_fpr_lr_stop.csv', 'a') as f:\n",
    "        f.write(f'{size},{biases[col]},{fprs[col]},{tprs[col]},{cdf.accuracy(pred, bow_validation[:, -1])}\\n')\n",
    "    \n",
    "\n",
    "# Save the FPRs (each row is a different vocabulary size, each column is a different bias. The first row contains the biases.\n",
    "fprs_arr = np.stack(fprs_sizes, axis=0)\n",
    "fprs_arr = np.vstack((biases, fprs_arr))\n",
    "np.savetxt('results/fprs_lr_stop.txt.gz', fprs_arr)\n",
    "\n",
    "# Save the TPRs (each row is a different vocabulary size, each column is a different bias. The first row contains the biases.\n",
    "tprs_arr = np.stack(tprs_sizes, axis=0)\n",
    "tprs_arr = np.vstack((biases, tprs_arr))\n",
    "np.savetxt('results/tprs_lr_stop.txt.gz', tprs_arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- SHOW THE ACCURACIES FOR DIFFERENT VOCABULARY SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the plot\n",
    "pd.read_csv('results/accuracies_lr_stop.csv').plot(x='vocsize', y=['accuracy_train', 'accuracy_validation'], kind='line', ylabel='Accuracy (%)', xlabel='Vocabulary size', title='Logistic Regression (stopwords)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- SHOW THE RESULTS ABOUT THE LOWEST FPR STORED ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the results related to the lowest fpr\n",
    "cdf.fpr_accs_vocsizes('results/low_fpr_lr_stop.csv', 'LR FPR and Accuracy vs. Vocsize (stopwords)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 - **Stopwords Kept** (file NOstop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- COMPUTE THE ACCURACY ON THE VALIDATION SET FOR DIFFERENT VOCABULARY SIZES\n",
    ">- COMPUTE THE ROC CURVE ON THE VALIDATION SET FOR DIFFERENT VOCABULARY SIZES\n",
    ">- STORE FOR EACH VOCABULARY SIZE THE FPR CLOSEST TO A GIVEN THRESHOLD AND THE ACCURACY ON THE VALIDATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocsizes\n",
    "vocsizes = [100, 200, 400, 800, 1000, 2000, 4000, 8000]\n",
    "fprs_sizes = []\n",
    "tprs_sizes = []\n",
    "fpr_threshold = -0.1\n",
    "\n",
    "# Define the range for the biases\n",
    "biases = np.linspace(-8,8,1000) #IF THE ROC CURVE IS NOT COMPLETE, JUST EXTEND THE RANGE\n",
    "\n",
    "# Create file where to store accs\n",
    "with open('results/accuracies_lr_NOstop.csv', 'w') as f: #overwrite the file\n",
    "    f.write('vocsize,accuracy_train,accuracy_validation\\n')\n",
    "    \n",
    "# Create file where to store data for the lowest fpr\n",
    "#with open('results/low_fpr_lr_NOstop.csv', 'w') as f:\n",
    "#    f.write('vocsize,bias,fpr,tpr,acc\\n')\n",
    "    \n",
    "for size in vocsizes:\n",
    "    size = str(size)\n",
    "\n",
    "    # Load the BoW for training (vstack on the two classes)\n",
    "    bow_train_bait = np.loadtxt('generated_gitignore/bow_train_bait_NOstop'+size+'.txt.gz')\n",
    "    bow_train_nobait = np.loadtxt('generated_gitignore/bow_train_nobait_NOstop'+size+'.txt.gz')\n",
    "    bow_train = np.vstack((bow_train_bait, bow_train_nobait))\n",
    "\n",
    "    bow_validation_bait = np.loadtxt('generated_gitignore/bow_validation_bait_NOstop'+size+'.txt.gz')\n",
    "    bow_validation_nobait = np.loadtxt('generated_gitignore/bow_validation_nobait_NOstop'+size+'.txt.gz')\n",
    "    bow_validation = np.vstack((bow_validation_bait, bow_validation_nobait))\n",
    "\n",
    "    # Load the Logistic Regression parameters\n",
    "    params = np.load('models_trained/param_logreg_NOstop'+size+'.npz')\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "\n",
    "    # Use the classifier to predict the labels of the validation set changing the vocabulary size\n",
    "    probs_val = cdf.logreg_inference(bow_validation[:, :-1], w, b)\n",
    "    predictions_val = (probs_val > 0.5).astype(int)\n",
    "    probs_tra = cdf.logreg_inference(bow_train[:, :-1], w, b)\n",
    "    predictions_tra = (probs_tra > 0.5).astype(int)\n",
    "\n",
    "    # Accuracy\n",
    "    with open('results/accuracies_lr_NOstop.csv', 'a') as f:\n",
    "        f.write(size+','+str(cdf.accuracy(predictions_tra, bow_train[:, -1]))+','+str(cdf.accuracy(predictions_val, bow_validation[:, -1]))+'\\n')\n",
    "    \n",
    "    # Compute the ROC curve\n",
    "    fprs, tprs, = cdf.roc_curve_biases(cdf.logreg_inference, bow_validation, w, biases, size, 'ROC curve LR for different biases', plot=True)\n",
    "    fprs_sizes.append(fprs)\n",
    "    tprs_sizes.append(tprs)\n",
    "    \n",
    "    # Find the biases associated with the lowest fpr\n",
    "    fprs_tmp = np.array(fprs)\n",
    "    col = np.where(fprs_tmp>fpr_threshold)[0][0]\n",
    "    prob = cdf.logreg_inference(bow_validation[:, :-1], w, biases[col])\n",
    "    pred = (prob > 0.5).astype(int)\n",
    "    \n",
    "    with open('results/low_fpr_lr_NOstop.csv', 'a') as f:\n",
    "        f.write(f'{size},{biases[col]},{fprs[col]},{tprs[col]},{cdf.accuracy(pred, bow_validation[:, -1])}\\n')\n",
    "    \n",
    "\n",
    "# Save the FPRs (each row is a different vocabulary size, each column is a different bias. The first row contains the biases.\n",
    "fprs_arr = np.stack(fprs_sizes, axis=0)\n",
    "fprs_arr = np.vstack((biases, fprs_arr))\n",
    "np.savetxt('results/fprs_lr_NOstop.txt.gz', fprs_arr)\n",
    "\n",
    "# Save the TPRs (each row is a different vocabulary size, each column is a different bias. The first row contains the biases.\n",
    "tprs_arr = np.stack(tprs_sizes, axis=0)\n",
    "tprs_arr = np.vstack((biases, tprs_arr))\n",
    "np.savetxt('results/tprs_lr_NOstop.txt.gz', tprs_arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- COMPUTE THE **PARAMETERS** FOR ALL THE VOCSIZES IN THE LIST AND STORE THEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the plot\n",
    "pd.read_csv('results/accuracies_lr_NOstop.csv').plot(x='vocsize', y=['accuracy_train', 'accuracy_validation'], kind='line', ylabel='Accuracy (%)', xlabel='Vocabulary size', title='Logistic Regression')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- SHOW THE RESULTS ABOUT THE LOWEST FPR STORED ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the results related to the lowest fpr\n",
    "cdf.fpr_accs_vocsizes('results/low_fpr_lr_NOstop.csv', 'LR FPR and Accuracy vs. Vocsize')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4 - **ANALYSIS OF THE BEST MODEL**\n",
    "\n",
    "- 4.1 - **Highest Accuracy:** MNBC without stopwords removed and vocabulary size 8000\n",
    "- 4.2 - **Lowest FPR:** MNBC without stopwords removed and vocabulary size 2000 biases = [5,-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the classifiers with respect to their accuracy on validation set\n",
    "\n",
    "df1 = pd.read_csv('results/accuracies_mnbc_stop.csv')\n",
    "df2 = pd.read_csv('results/accuracies_mnbc_NOstop.csv')\n",
    "df3 = pd.read_csv('results/accuracies_lr_stop.csv')\n",
    "df4 = pd.read_csv('results/accuracies_lr_NOstop.csv')\n",
    "df = df1.join(df2, lsuffix='_mnbc_stop', rsuffix='_mnbc_NOstop')\n",
    "df = df.join(df3, rsuffix='_lr_stop')\n",
    "df = df.join(df4, rsuffix='_lr_NOstop')\n",
    "df = df.drop(columns=['vocsize', 'vocsize_lr_NOstop', 'vocsize_mnbc_NOstop'])\n",
    "df = df.rename(columns={'accuracy_train': 'accuracy_train_lr_stop', 'accuracy_validation':'accuracy_validation_lr_stop'})\n",
    "df.plot(kind='bar', x='vocsize_mnbc_stop', y=['accuracy_validation_mnbc_stop', 'accuracy_validation_mnbc_NOstop', 'accuracy_validation_lr_stop', 'accuracy_validation_lr_NOstop'], ylabel='Accuracy (%)', xlabel='Vocabulary size', title='Comparison of the classifiers')\n",
    "plt.legend(bbox_to_anchor=(1.08, -0.115), ncols = 2)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "\n",
    "# Compare the classifiers with respect to their trade-off between accuracy and FPR (False Positive Rate) on validation set \n",
    "# The best is the MNBC with the stopwords kept and the results for the vocsizes are shown below\n",
    "\n",
    "#display the results related to the lowest fpr\n",
    "cdf.fpr_accs_vocsizes('results/low_fpr_mnbc_NOstop.csv', 'MNBC FPR and Accuracy vs. Vocsize')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 - **Highest Accuracy model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the features, the vocabulary and train the model once for all\n",
    "\n",
    "#load vocabulary\n",
    "voc_stopwords = np.loadtxt('generated_gitignore/vocabulary_NOstop8000.txt', dtype=str)\n",
    "\n",
    "#getting features \n",
    "bow1 = np.loadtxt('generated_gitignore/bow_test_bait_NOstop8000.txt.gz')\n",
    "bow2 = np.loadtxt('generated_gitignore/bow_test_nobait_NOstop8000.txt.gz')\n",
    "bow_test = np.vstack((bow1, bow2))\n",
    "\n",
    "bow1 = np.loadtxt('generated_gitignore/bow_train_bait_NOstop8000.txt.gz')\n",
    "bow2 = np.loadtxt('generated_gitignore/bow_train_nobait_NOstop8000.txt.gz')\n",
    "bow_train = np.vstack((bow1, bow2))\n",
    "\n",
    "#training algorithm \n",
    "w, b = pvml.multinomial_naive_bayes_train(bow_train[:, :-1], bow_train[:, -1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- IDENTIFY THE MOST IMPACTFUL WORDS\n",
    "\n",
    "> The most impactful words for a class are computed as the words probability gap between the two classes is the largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the indices of the sorted array\n",
    "delta = w[:,0] - w[:,1]\n",
    "indices = delta.argsort()[::-1]\n",
    "\n",
    "#print results and store results for easier insertion in latex\n",
    "with open('results/impactful_words_mnbc_NOstop8000.txt', 'w') as f:\n",
    "    print(\"NEGATIVE CLASS\", file = f)\n",
    "    for i in indices[:10]:\n",
    "        print(voc_stopwords[i], delta[i], file = f)\n",
    "\n",
    "    print('', file = f)\n",
    "    print(\"POSITIVE CLASS\", file = f)\n",
    "    for i in indices[-1:-11:-1]:\n",
    "        print(voc_stopwords[i], delta[i], file = f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- INDENTIFY THE WORST ERRORS\n",
    "\n",
    ">The worst errors are the misclassified headlines whose score gap between the two classes is the largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the headlines\n",
    "hlines = open('data/clickbait_test.txt').read().strip().split('\\n')\n",
    "hlines_nobait = open('data/non_clickbait_test.txt').read().strip().split('\\n')\n",
    "hlines.extend(hlines_nobait)\n",
    "\n",
    "# Use the classifier to predict the labels of the validation set changing the vocabulary size\n",
    "predictions_test, scores_test = pvml.multinomial_naive_bayes_inference(bow_test[:, :-1], w, b)\n",
    "\n",
    "delta = scores_test[:,1] - scores_test[:,0]\n",
    "c = pd.DataFrame({'headlines': hlines, 'delta': delta})\n",
    "c_full = c.copy()\n",
    "c_corr = c[predictions_test == bow_test[:, -1]]\n",
    "c = c[predictions_test != bow_test[:, -1]]\n",
    "c.sort_values(by='delta', ascending=False, inplace=True)\n",
    "false_positive = c.head(10)\n",
    "false_negative = c.tail(10)\n",
    "false_positive.plot(x = 'headlines', y = 'delta', kind = 'bar', title = 'False positive headlines (MNBC 8000)', ylabel = 'Score', xlabel = 'headline content', legend = False)\n",
    "false_negative.plot(x = 'headlines', y = 'delta', kind = 'bar', title = 'False negative headlines (MNBC 8000)', ylabel = 'Score', xlabel = 'headline content', legend = False)\n",
    "\n",
    "# Compute avg length of the headlines misclassified and compare it with the avg length of the headlines\n",
    "c['length'] = c['headlines'].apply(lambda x: len(x.split()))\n",
    "print('Avg length of misclassified: ', c['length'].mean())\n",
    "c_full['length'] = c_full['headlines'].apply(lambda x: len(x.split()))\n",
    "print('Avg length of all headlines: ', c_full['length'].mean())\n",
    "c_corr['length'] = c_corr['headlines'].apply(lambda x: len(x.split()))\n",
    "print('Avg length of correctly classified: ', c_corr['length'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(['misclassified', 'all', 'correctly'], [c['length'].mean(), c_full['length'].mean(), c_corr['length'].mean()], align='center', width=0.5)\n",
    "plt.title('Avg length of headlines')\n",
    "plt.ylabel('Avg length')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- FINAL PRINT OF THE ACCURACIES ON THE TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracies of test and training\n",
    "predictions_test, scores_test = pvml.multinomial_naive_bayes_inference(bow_test[:, :-1], w, b)\n",
    "predictions_train, scores_train = pvml.multinomial_naive_bayes_inference(bow_train[:, :-1], w, b)\n",
    "\n",
    "print('Test accuracy: ', cdf.accuracy(predictions_test, bow_test[:, -1]))\n",
    "print('Train accuracy: ', cdf.accuracy(predictions_train, bow_train[:, -1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 - **Lowest FPR model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the features, the vocabulary and train the model once for all\n",
    "\n",
    "#load vocabulary\n",
    "voc_stopwords = np.loadtxt('generated_gitignore/vocabulary_NOstop2000.txt', dtype=str)\n",
    "\n",
    "#getting features \n",
    "bow1 = np.loadtxt('generated_gitignore/bow_test_bait_NOstop2000.txt.gz')\n",
    "bow2 = np.loadtxt('generated_gitignore/bow_test_nobait_NOstop2000.txt.gz')\n",
    "bow_test = np.vstack((bow1, bow2))\n",
    "\n",
    "bow1 = np.loadtxt('generated_gitignore/bow_train_bait_NOstop2000.txt.gz')\n",
    "bow2 = np.loadtxt('generated_gitignore/bow_train_nobait_NOstop2000.txt.gz')\n",
    "bow_train = np.vstack((bow1, bow2))\n",
    "\n",
    "#training algorithm \n",
    "w, b = pvml.multinomial_naive_bayes_train(bow_train[:, :-1], bow_train[:, -1])\n",
    "b = [5, -5] #set the optimal biases for the lowest fpr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- IDENTIFY THE MOST IMPACTFUL WORDS\n",
    "\n",
    "> The most impactful words for a class are computed as the words whose score gap between the two classes is the largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the indices of the sorted array\n",
    "delta = w[:,0] - w[:,1]\n",
    "indices = delta.argsort()[::-1]\n",
    "\n",
    "#print results and store results for easier insertion in latex\n",
    "with open('results/impactful_words_mnbc_NOstop2000.txt', 'w') as f:\n",
    "    print(\"NEGATIVE CLASS\", file = f)\n",
    "    for i in indices[:10]:\n",
    "        print(voc_stopwords[i], delta[i], file = f)\n",
    "\n",
    "    print('', file = f)\n",
    "    print(\"POSITIVE CLASS\", file = f)\n",
    "    for i in indices[-1:-11:-1]:\n",
    "        print(voc_stopwords[i], delta[i], file = f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- INDENTIFY THE WORST ERRORS\n",
    "\n",
    ">The worst errors are the misclassified headlines whose score gap between the two classes is the largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the headlines\n",
    "hlines = open('data/clickbait_test.txt').read().strip().split('\\n')\n",
    "hlines_nobait = open('data/non_clickbait_test.txt').read().strip().split('\\n')\n",
    "hlines.extend(hlines_nobait)\n",
    "\n",
    "# Use the classifier to predict the labels of the validation set changing the vocabulary size\n",
    "predictions_test, scores_test = pvml.multinomial_naive_bayes_inference(bow_test[:, :-1], w, b)\n",
    "\n",
    "delta = scores_test[:,1] - scores_test[:,0]\n",
    "c = pd.DataFrame({'headlines': hlines, 'delta': delta})\n",
    "c = c[predictions_test != bow_test[:, -1]]\n",
    "c.sort_values(by='delta', ascending=False, inplace=True)\n",
    "false_positive = c.head(10)\n",
    "false_negative = c.tail(10)\n",
    "false_positive.plot(x = 'headlines', y = 'delta', kind = 'bar', title = '*False positive* headlines (MNBC 2000)', ylabel = 'Score', xlabel = 'headline content', legend = False)\n",
    "false_negative.plot(x = 'headlines', y = 'delta', kind = 'bar', title = 'False negative headlines (MNBC 2000)', ylabel = 'Score', xlabel = 'headline content', legend = False)\n",
    "\n",
    "# Compute avg length of the headlines misclassified and compare it with the avg length of the headlines\n",
    "c['length'] = c['headlines'].apply(lambda x: len(x.split()))\n",
    "print('Avg length of misclassified: ', c['length'].mean())\n",
    "c_full['length'] = c_full['headlines'].apply(lambda x: len(x.split()))\n",
    "print('Avg length of all headlines: ', c_full['length'].mean())\n",
    "c_corr['length'] = c_corr['headlines'].apply(lambda x: len(x.split()))\n",
    "print('Avg length of correctly classified: ', c_corr['length'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(['misclassified', 'all', 'correctly'], [c['length'].mean(), c_full['length'].mean(), c_corr['length'].mean()], align='center', width=0.5)\n",
    "plt.title('Avg length of headlines')\n",
    "plt.ylabel('Avg length')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE BLOCK:\n",
    "\n",
    ">- FINAL PRINT OF THE ACCURACIES ON THE TEST SET AND THE FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracies of test and training\n",
    "predictions_test, scores_test = pvml.multinomial_naive_bayes_inference(bow_test[:, :-1], w, b)\n",
    "predictions_train, scores_train = pvml.multinomial_naive_bayes_inference(bow_train[:, :-1], w, b)\n",
    "\n",
    "print('Test accuracy: ', cdf.accuracy(predictions_test, bow_test[:, -1]))\n",
    "print('Train accuracy: ', cdf.accuracy(predictions_train, bow_train[:, -1]))\n",
    "\n",
    "confmat = skm.confusion_matrix(bow_test[:, -1], predictions_test)\n",
    "tn, fp, fn, tp = confmat.ravel()\n",
    "print('The FPR (%) is: ', fp/(fp+tn)*100)\n",
    "print('The TPR (%) is: ', tp/(tp+fn)*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **DETAILS**: Deep Training of the Logistic Regression model\n",
    "\n",
    "Train the potentially best model with respect to the accuracy and analyze its accuracy and fpr on the test set: LR without stopwords and vocsize 4000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BoW for training (vstack on the two classes)\n",
    "size = str(4000)\n",
    "bow1 = np.loadtxt('generated_gitignore/bow_train_bait_NOstop'+size+'.txt.gz')\n",
    "bow2 = np.loadtxt('generated_gitignore/bow_train_nobait_NOstop'+size+'.txt.gz')\n",
    "bow_train = np.vstack((bow1, bow2))\n",
    "\n",
    "bow1 = np.loadtxt('generated_gitignore/bow_test_bait_NOstop'+size+'.txt.gz')\n",
    "bow2 = np.loadtxt('generated_gitignore/bow_test_nobait_NOstop'+size+'.txt.gz')\n",
    "bow_test = np.vstack((bow1, bow2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_ = 20000 #about 10x larger than the number of iterations needed to see a graphically rough converge\n",
    "\n",
    "# Train the Logistic Regression\n",
    "w, b, train_accuracies, test_accuracies, losses, iter_to_conv = cdf.logreg_training(bow_train[:, :-1], bow_train[:, -1], steps_, 0.001, 0.2, tol=0.0000005, X_test=bow_test[:, :-1], Y_test=bow_test[:, -1])\n",
    "\n",
    "# Save the parameters\n",
    "np.savez('models_trained/param_logreg_NOstop'+size+'_deeptrain.npz', w=w, b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "accs.append(train_accuracies)\n",
    "accs.append(test_accuracies)\n",
    "accs.append(losses)\n",
    "np.savetxt('results/train_test_losses_NOstop'+size+'_deeptrain.txt.gz', np.stack(accs, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.loadtxt('results/train_test_losses_NOstop4000_deeptrain.txt.gz')\n",
    "train_accuracies = results[0,:]\n",
    "test_accuracies = results[1,:]\n",
    "losses = results[2,:]\n",
    "\n",
    "plt.plot(train_accuracies*100, label='train')\n",
    "plt.plot(test_accuracies*100, label='test')\n",
    "plt.title('Accuracy vs. iterations LR 4000')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('iterations (x100)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(losses, label='loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations (x100)')\n",
    "plt.title('Loss vs. iterations LR 4000')\n",
    "plt.show()\n",
    "\n",
    "#print('The loss threshold has been reached at iteration: ',iter_to_conv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
